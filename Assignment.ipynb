{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYhSC_TzJhZH"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import warnings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrEwg_tZJjYQ",
        "outputId": "59209f6d-388f-46d1-aa7e-cfbb88974a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('/content/drive/My Drive/Colab Notebooks/audio-project.txt' ,'r' , errors='ignore')\n",
        "text=f.read()\n",
        "stopwords=list(STOP_WORDS)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(text)\n",
        "tokens= [token.text for token in doc]\n",
        "\n",
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "    if word.text.lower() not in stopwords:\n",
        "        if word.text.lower() not in punctuation:\n",
        "            if word.text not in word_frequencies.keys():\n",
        "                word_frequencies[word.text]=1\n",
        "            else:\n",
        "                word_frequencies[word.text]+=1\n",
        "                \n",
        "            \n",
        "                \n",
        "                max_frequency=max(word_frequencies.values())\n",
        "                \n",
        "              \n",
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word]=word_frequencies[word]/max_frequency\n",
        "    \n",
        "sentence_tokens = [ sent for sent in doc.sents] \n",
        "\n",
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "    for word in sent: \n",
        "        if word.text.lower() in word_frequencies.keys():\n",
        "            if sent not in sentence_scores.keys():\n",
        "                sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
        "            else:\n",
        "                sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
        "               \n",
        "from heapq import nlargest\n",
        "\n",
        "select_length = int(len(sentence_tokens)*0.3)\n",
        " \n",
        "summary = nlargest(select_length , sentence_scores , key=sentence_scores.get)\n",
        "\n",
        "final_summary=[word.text for word in summary]\n",
        "summary = ''.join(final_summary)               \n",
        " \n",
        "print(len(text))\n",
        "print(len(summary))   \n",
        "print(summary)"
      ],
      "metadata": {
        "id": "4AN174K2JmfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}